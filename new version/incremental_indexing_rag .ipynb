{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c1ad72",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Incremental Indexing\n",
    "This notebook demonstrates how to combine indexing and RAG pipelines to process URLs one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829a8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in c:\\users\\xdn13\\appdata\\roaming\\python\\python311\\site-packages (2.18.0)\n",
      "Requirement already satisfied: docstring-parser in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.17.0)\n",
      "Requirement already satisfied: filetype in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: haystack-experimental in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.13.0)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.25.1)\n",
      "Requirement already satisfied: lazy-imports in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.0.1)\n",
      "Requirement already satisfied: more-itertools in d:\\python3.11\\lib\\site-packages (from haystack-ai) (10.8.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.5)\n",
      "Requirement already satisfied: numpy in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.3.3)\n",
      "Requirement already satisfied: openai>=1.56.1 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.107.2)\n",
      "Requirement already satisfied: posthog!=3.12.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.7.4)\n",
      "Requirement already satisfied: pydantic in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.11.7)\n",
      "Requirement already satisfied: python-dateutil in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (9.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\python3.11\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.56.1->haystack-ai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\python3.11\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (2.4.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->haystack-ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.25.1)\n",
      "Requirement already satisfied: datasets>=3.6.0 in d:\\python3.11\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.33.0)\n",
      "Requirement already satisfied: packaging in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.12.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\python3.11\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets>=3.6.0) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.6.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.6.0) (1.17.0)\n",
      "Requirement already satisfied: sentence-transformers>=4.1.0 in d:\\python3.11\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.52.4)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.7.0)\n",
      "Requirement already satisfied: scipy in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (0.33.0)\n",
      "Requirement already satisfied: Pillow in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=4.1.0) (2025.3.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=4.1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->sentence-transformers>=4.1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=4.1.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (3.6.0)\n",
      "Requirement already satisfied: accelerate in d:\\python3.11\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\python3.11\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "!pip install haystack-ai\n",
    "!pip install \"datasets>=3.6.0\"\n",
    "!pip install \"sentence-transformers>=4.1.0\"\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.components.embedders.cohere import CohereDocumentEmbedder, CohereTextEmbedder\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator,FaithfulnessEvaluator\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack_integrations.components.rankers.cohere import CohereRanker\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ä».envæ–‡ä»¶è¯»å–API key\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = cohere_api_key if cohere_api_key else \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key if openai_api_key else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a98202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# è¯»å–æœ¬åœ° index_table.json æ–‡ä»¶\n",
    "with open(\"../files/index_table.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0e93",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d5e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a privacy policy expert. You are provided with {{app_url}}, which contains the privacy policy document for an app.\n",
    "Your task is to:\n",
    " - answer the question based on the privacy policy document,\n",
    " - provide references for your answers based on the section in the privacy policy document from which your answer is generated,\n",
    " - produce your results strictly in the JSON format below (no extra text beyond JSON),\n",
    " - ensure that the 'url' in the 'meta' section is exactly {{app_url}}.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "   \"meta\": {\n",
    "       \"id\": {{ app_id }},\n",
    "       \"url\": {{ app_url }},\n",
    "       \"title\": {{ app_name | tojson }}\n",
    "   },\n",
    "   \"reply\": {\n",
    "       \"qid\": \"{{ qid }}\",\n",
    "       \"question\": \"{{ question | escape }}\",\n",
    "       \"answer\": {\n",
    "           \"full_answer\": \"{{ full_answer | escape }}\",\n",
    "           \"simple_answer\": \"{{ simple_answer | escape }}\",\n",
    "           \"extended_simple_answer\": {\n",
    "               \"comment\": \"{{ extended_comment | escape }}\",\n",
    "               \"content\": \"{{ extended_content | escape }}\"\n",
    "           }\n",
    "       },\n",
    "       \"analysis\": \"{{ analysis | escape }}\",\n",
    "       \"reference\": \"{{ reference | escape }}\"\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Approach each question systematically:\n",
    "   a. Understand the question: Break down the question into specific components or sub-questions if needed.\n",
    "\n",
    "   b. Identify relevant context from the privacy policy.\n",
    "   c. Analyze the context and link it back to the question.\n",
    "   d. Formulate the answer for each JSON field.\n",
    "   e. Provide references:  (original text + 'URL: {{app_url}}'). If none, report 'N/A. URL: {{app_url}}'. Note that the original text must come from only the relevant context in page {{app_url}}\n",
    "\n",
    "2. Output structure:\n",
    "   - full_answer: must integrate info from both simple_answer and extended_simple_answer. The full answer section must not be empty.\n",
    "   - simple_answer: follow the rules above. This section must not be empty.\n",
    "   - extended_simple_answer: follow the rules above, or empty if not specified\n",
    "   - analysis: describe your reasoning\n",
    "   - reference: original text snippets + URL. The context must come from the app URL {{app_url}}. Attach the {{app_url}} in the end. Ensure JSON compatibility by replacing double quotes with single quotes.\n",
    "\n",
    "\n",
    "3. Special rules for the `simple_answer` and `extended_simple_answer` fields:\n",
    "   - If the question is â€œ1. Does the app declare the collection of data?â€:\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is â€œ2. If the app declares the collection of data, what type of data does it collect?':\n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer: \n",
    "           - comment: \"data collected\"\n",
    "           - content: list of data types collected\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "   - If the question is \"3. Does the app declare the purpose of data collection and use?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"4. Can the user opt out of data collection or delete data?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"5. Does the app share data with third parties?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"6. If the app shares data with third parties, what third parties does the app share data with?\": \n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer:\n",
    "           - comment: \"third parties\"\n",
    "           - content: list of third parties\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "  URL: {{ doc.meta['url'] }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'outputs/'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436c1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xdn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize document store and pipelines\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# ===================== é™æ€æ— çŠ¶æ€ç»„ä»¶ï¼ˆå¾ªç¯å¤–å»ºä¸€æ¬¡ï¼‰ =====================\n",
    "# ä¸ºé˜²æ­¢éƒ¨åˆ†ç«™ç‚¹æ‹’ç»é»˜è®¤UAï¼Œæ·»åŠ æµè§ˆå™¨UAï¼Œå¹¶åœ¨å¤±è´¥æ—¶ä¸æŠ›å‡ºå¼‚å¸¸\n",
    "fetcher = LinkContentFetcher(raise_on_failure=False)\n",
    "converter = HTMLToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "\n",
    "splitter = DocumentSplitter(\n",
    "    split_by=\"word\",\n",
    "    split_length=220,\n",
    "    split_overlap=50\n",
    ")\n",
    "\n",
    "embedder = CohereDocumentEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "query_embedder = CohereTextEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "\n",
    "prompt_builder = PromptBuilder(\n",
    "    template=prompt,\n",
    "    required_variables=[\"query\", \"app_id\", \"app_url\", \"question\", \"documents\"]\n",
    ")\n",
    "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
    "answer_builder = AnswerBuilder()\n",
    "\n",
    "\n",
    "reranker = CohereRanker(model=\"rerank-english-v3.0\", top_k=5)\n",
    "\n",
    "\n",
    "# ===================== å ä½çš„å¯å˜ç»„ä»¶ï¼ˆå…ˆç»‘å®šä¸€ä¸ªä¸´æ—¶ storeï¼‰ =====================\n",
    "# å…³é”®ï¼šwriter / retriever çš„å®ä¾‹æ˜¯â€œå¯å˜â€çš„ï¼Œæˆ‘ä»¬æ¯è½®æ›¿æ¢å®ƒä»¬çš„ document_store å³å¯\n",
    "_initial_store = InMemoryDocumentStore()\n",
    "writer = DocumentWriter(document_store=_initial_store, policy=DuplicatePolicy.OVERWRITE)\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=_initial_store, top_k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d99054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001A86182F450>\n",
       "ğŸš… Components\n",
       "  - query_embedder: CohereTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - reranker: CohereRanker\n",
       "  - prompt: PromptBuilder\n",
       "  - generator: OpenAIGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "ğŸ›¤ï¸ Connections\n",
       "  - query_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> reranker.documents (list[Document])\n",
       "  - reranker.documents -> prompt.documents (List[Document])\n",
       "  - reranker.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt.prompt -> generator.prompt (str)\n",
       "  - generator.replies -> answer_builder.replies (list[str])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================== Pipeline åªæ„å»ºä¸€æ¬¡ï¼ˆæ—  Noneï¼‰ =====================\n",
    "# ç´¢å¼•ç®¡é“ï¼šfetch -> convert -> clean -> split -> embed -> write\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"fetcher\", fetcher)\n",
    "indexing.add_component(\"converter\", converter)\n",
    "indexing.add_component(\"cleaner\", cleaner)\n",
    "indexing.add_component(\"splitter\", splitter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)  # æ³¨æ„ï¼šæ˜¯çœŸå® writerï¼Œä¸æ˜¯ None\n",
    "\n",
    "indexing.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "indexing.connect(\"converter\", \"cleaner\")\n",
    "indexing.connect(\"cleaner\", \"splitter\")\n",
    "indexing.connect(\"splitter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")\n",
    "\n",
    "# RAG ç®¡é“ï¼šquery_embedder -> retriever -> prompt -> generator -> answer_builder\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"query_embedder\", query_embedder)\n",
    "rag.add_component(\"retriever\", retriever)  # æ³¨æ„ï¼šæ˜¯çœŸå® retrieverï¼Œä¸æ˜¯ None\n",
    "rag.add_component(\"reranker\", reranker)    # âœ… æ–°å¢é‡æ’å™¨\n",
    "rag.add_component(\"prompt\", prompt_builder)\n",
    "rag.add_component(\"generator\", generator)\n",
    "rag.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# âœ… å…ˆå¬å› â†’ å†é‡æ’ â†’ å†é€ç»™ prompt / answer_builder\n",
    "rag.connect(\"retriever.documents\", \"reranker.documents\")\n",
    "rag.connect(\"reranker.documents\", \"prompt.documents\")\n",
    "rag.connect(\"reranker.documents\", \"answer_builder.documents\")\n",
    "rag.connect(\"prompt\", \"generator\")\n",
    "rag.connect(\"generator.replies\", \"answer_builder.replies\")\n",
    "# ï¼ˆä¸è¦è¿ query_embedder.text -> answer_builder.queryï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c621",
   "metadata": {},
   "source": [
    "## Use LLM generate replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c9ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"1. Does the app declare the collection of data?\",\n",
    "    \"2. If the app declares the collection of data, what type of data does it collect?\",\n",
    "    \"3. Does the app declare the purpose of data collection and use?\",\n",
    "    \"4. Can the user opt out of data collection or delete data?\",\n",
    "    \"5. Does the app share data with third parties?\",\n",
    "    \"6. If the app shares data with third parties, what third parties does the app share data with?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ===================== æ£€ç´¢çŸ­æŸ¥è¯¢æ˜ å°„ï¼ˆStep B æ–°å¢ï¼‰ =====================\n",
    "# è¯´æ˜ï¼šè¿™äº›æ˜¯â€œç»™å‘é‡æ£€ç´¢ç”¨çš„çŸ­è¯­â€ï¼Œå°½é‡å»æ‰æ¨¡æ¿è¯ï¼Œåªä¿ç•™è¯­ä¹‰æ ¸å¿ƒï¼Œæš‚æ—¶ä¸ç”¨\n",
    "query_map = {\n",
    "    \"1. Does the app declare the collection of data?\": \"data collection\",\n",
    "    \"2. What type of data does it collect?\": \"type of data collected\",\n",
    "    \"3. Does the app declare the purpose of data collection and use?\": \"purpose of data collection\",\n",
    "    \"4. Can you opt out of data collection or delete data?\": \"opt out of data collection\",\n",
    "    \"5. Does the app share data with third parties?\": \"data sharing with third parties\",\n",
    "    \"6. If the app shares data with third parties, what third parties does the app share data with?\": \"which third parties receive data\",\n",
    "}\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºæ–‡ä»¶å¤¹\n",
    "output_folder = 'outputs/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# è®©ç”¨æˆ·å¯è‡ªå®šä¹‰é€‰å–æ•°é‡\n",
    "num_to_process = 1  # ä¿®æ”¹æ­¤å¤„å³å¯è®¾å®šå¤„ç†å‰å‡ ä¸ªURL\n",
    "\n",
    "# æ–‡æ¡£æ‘˜è¦é•¿åº¦è®¾å®š\n",
    "MAX_DOC_EXCERPT = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12d7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…œåº•å¯¹è±¡ answer_json æ„é€ å®Œä¹‹åï¼Œç«‹åˆ»åŠ ä¸Šè¿™æ®µâ€œäºŒæ¬¡ä¿®å¤â€\n",
    "import re, json\n",
    "\n",
    "def _try_salvage_nested_json(answer_json):\n",
    "    # ä»…åœ¨ full_answer æ˜¯å­—ç¬¦ä¸²ã€ä¸”åƒ JSON æ—¶å°è¯•\n",
    "    raw = (answer_json.get(\"reply\", {}).get(\"answer\", {}).get(\"full_answer\") or \"\").strip()\n",
    "    if not (isinstance(raw, str) and raw.startswith(\"{\")):\n",
    "        return answer_json\n",
    "\n",
    "    # 1) å»æ‰å¸¸è§çš„â€œç»“å°¾å¤šé€—å·â€\n",
    "    fixed = re.sub(r\",\\s*([}\\]])\", r\"\\1\", raw)\n",
    "    # 2) å»æ‰å›´æ  ```json ... ```\n",
    "    fixed = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", fixed)\n",
    "\n",
    "    try:\n",
    "        cand = json.loads(fixed)\n",
    "        # è‹¥ cand æœ¬èº«å°±æ˜¯æˆ‘ä»¬æœŸå¾…çš„ç»“æ„ï¼ˆå« reply/answerï¼‰ï¼Œç›´æ¥ç”¨å®ƒæ›¿æ¢\n",
    "        if isinstance(cand, dict) and \"reply\" in cand and \"answer\" in cand.get(\"reply\", {}):\n",
    "            # ä¿ç•™ç°æœ‰ qidï¼Œé¿å…ä¸¢é¢˜å·\n",
    "            qid = answer_json.get(\"reply\", {}).get(\"qid\")\n",
    "            cand.setdefault(\"reply\", {}).setdefault(\"qid\", qid)\n",
    "            return cand\n",
    "    except Exception:\n",
    "        pass\n",
    "    return answer_json\n",
    "\n",
    "\n",
    "# ç”Ÿæˆå€™é€‰æŠ“å–URLï¼ˆhttp/httpsã€æœ«å°¾æ–œæ ã€å¸¸è§éšç§è·¯å¾„ï¼‰\n",
    "def _generate_candidate_urls(url: str):\n",
    "     candidates = []\n",
    "     try:\n",
    "         parsed = urlparse(url)\n",
    "         base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "         # åŸå§‹\n",
    "         candidates.append(url)\n",
    "         # æœ«å°¾æ–œæ å˜ä½“\n",
    "         if not url.endswith(\"/\"):\n",
    "             candidates.append(url + \"/\")\n",
    "         # http å˜ä½“\n",
    "         if parsed.scheme.lower() == \"https\":\n",
    "             candidates.append(url.replace(\"https://\", \"http://\", 1))\n",
    "             if not url.endswith(\"/\"):\n",
    "                 candidates.append((url + \"/\").replace(\"https://\", \"http://\", 1))\n",
    "         # å¸¸è§éšç§è·¯å¾„ï¼ˆå¦‚æœç»™çš„æ˜¯ç«™ç‚¹é¦–é¡µæˆ–é”™è¯¯è·¯å¾„ï¼‰\n",
    "         common_paths = [\n",
    "             \"/privacy\", \"/privacy/\", \"/privacy-policy\", \"/privacy-policy/\",\n",
    "             \"/privacy-policy.html\", \"/privacypolicy\", \"/privacypolicy/\",\n",
    "             \"/privacypolicy.html\"\n",
    "         ]\n",
    "         for p in common_paths:\n",
    "             candidates.append(base + p)\n",
    "     except Exception:\n",
    "         pass\n",
    "     # å»é‡ä¿åº\n",
    "     seen = set()\n",
    "     uniq = []\n",
    "     for u in candidates:\n",
    "         if u and u not in seen:\n",
    "             uniq.append(u)\n",
    "             seen.add(u)\n",
    "     return uniq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdecb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Document 1/1: None (1361356590)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.53it/s]\n",
      "|Processing Questions for None:  17%|â–ˆâ–‹        | 1/6 [00:04<00:21,  4.27s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        1. Does the app declare the collection of data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:07<00:14,  3.62s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        2. If the app declares the collection of data, what type of data does it collect?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:10<00:10,  3.34s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        3. Does the app declare the purpose of data collection and use?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:14<00:06,  3.44s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        4. Can the user opt out of data collection or delete data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:18<00:03,  3.78s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        5. Does the app share data with third parties?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:21<00:00,  3.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        6. If the app shares data with third parties, what third parties does the app share data with?\n",
      "        \n",
      "\n",
      "Saved answers for App ID 1361356590 to outputs/1361356590.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== ä¸»å¾ªç¯ï¼šæ¯ä¸ª App ç‹¬ç«‹ storeï¼Œä½† pipeline ä¸é‡å»º =====================\n",
    "for idx, item in enumerate(data[:num_to_process]):\n",
    "    app_id = item.get('id')\n",
    "    app_name = item.get('title')\n",
    "    app_url = item.get('url')\n",
    "    if not app_url:\n",
    "        print(f\"No URL found for App ID: {app_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing Document {idx + 1}/{num_to_process}: {app_name} ({app_id})\")\n",
    "\n",
    "    # â€”â€” æ ¸å¿ƒï¼šä¸ºå½“å‰ App åˆ›å»ºç‹¬ç«‹çš„ storeï¼Œå¹¶â€œå°±åœ°æ›¿æ¢â€ writer/retriever æ‰€å¼•ç”¨çš„ store â€”â€” #\n",
    "    current_store = InMemoryDocumentStore()\n",
    "    writer.document_store = current_store          # ç´¢å¼•å†™å…¥åˆ°å½“å‰ store\n",
    "    retriever.document_store = current_store       # æ£€ç´¢ä»å½“å‰ store è¯»å–\n",
    "\n",
    "\n",
    "    # ç´¢å¼•å½“å‰ app çš„é¡µé¢ï¼ˆå«å¤šURLå›é€€ï¼Œé¿å… 400/403/404 ç›´æ¥å¤±è´¥ï¼‰\n",
    "    success = False\n",
    "    for try_url in _generate_candidate_urls(app_url):\n",
    "        try:\n",
    "            _ = indexing.run({\"fetcher\": {\"urls\": [try_url]}})\n",
    "            # è‹¥å†™å…¥æˆåŠŸåº”æœ‰æ–‡æ¡£\n",
    "            if getattr(current_store, \"count_documents\", None):\n",
    "                if current_store.count_documents() > 0:\n",
    "                    success = True\n",
    "                    if try_url != app_url:\n",
    "                        print(f\"[indexing] Fallback URL used: {try_url}\")\n",
    "                    break\n",
    "            else:\n",
    "                # ä¸æ”¯æŒè®¡æ•°åˆ™ä»¥ä¸æŠ¥é”™è§†ä¸ºæˆåŠŸ\n",
    "                success = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[indexing] Failed on {try_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not success:\n",
    "        print(f\"âš ï¸ Unable to fetch any content for App ID {app_id}. Skipping this app.\")\n",
    "        continue\n",
    "\n",
    "    answers_list = []\n",
    "\n",
    "    for j in tqdm(range(len(questions)), desc=f\"|Processing Questions for {app_name}\", unit=\"question\"):\n",
    "        query = f\"\"\"\n",
    "        You are analyzing the '{app_name}' with URL: {app_url}.\n",
    "        Answer the following questions based on the privacy policy document:\n",
    "        {questions[j]}\n",
    "        \"\"\"\n",
    "        # Step B: ä¸ºæ£€ç´¢æ„é€ â€œçŸ­æŸ¥è¯¢â€ â€”â€” æ›´èšç„¦ã€æ›´ç¨³å¥\n",
    "        retriever_query = query_map.get(j, questions[j])\n",
    "\n",
    "        result = rag.run({\n",
    "            # âœ… ç”¨çŸ­æŸ¥è¯¢è¿›è¡Œå‘é‡å¬å›\n",
    "            \"query_embedder\": {\"text\": questions[j]},\n",
    "            # âœ… æ”¾å¤§å¬å›æ± ï¼›é‡æ’å™¨ä¼šå‹åˆ° top_k=5ï¼ˆè§ä¸Šé¢çš„ reranker.top_kï¼‰\n",
    "            \"retriever\": {\"top_k\": 15},\n",
    "            \"reranker\": {\"query\": questions[j]},  # âœ… ä¸º CohereRanker æä¾›å¿…éœ€çš„ query\n",
    "            \"prompt\": {\"qid\": f\"q{j+1}\", \"query\": query, \"app_id\": app_id, \"app_url\": app_url, \"question\": questions[j]},\n",
    "            \"answer_builder\": {\"query\": query}\n",
    "        })\n",
    "\n",
    "        generated_answers = result['answer_builder']['answers']\n",
    "\n",
    "        source_docs_export = []\n",
    "        retrieved_context_list = []\n",
    "\n",
    "        if generated_answers:\n",
    "            structured_answer = generated_answers[0]\n",
    "            answer = structured_answer.data\n",
    "            source_documents = structured_answer.documents\n",
    "\n",
    "            print(f\"Question {j+1} answered using {len(source_documents)} document (forced single)\")\n",
    "            print(f\"Query: {structured_answer.query}\")\n",
    "\n",
    "            for d in source_documents:\n",
    "                excerpt = (d.content or \"\")[:500]\n",
    "                if len(d.content or \"\") > 500: excerpt += \"...\"\n",
    "                source_docs_export.append({\n",
    "                    \"id\": getattr(d, 'id', None),\n",
    "                    \"score\": getattr(d, 'score', None),\n",
    "                    \"excerpt\": excerpt,\n",
    "                    \"url\": (getattr(d, 'meta', {}) or {}).get('url')\n",
    "                })\n",
    "                retrieved_context_list.append(excerpt)\n",
    "        else:\n",
    "            # å›é€€åˆ°ç›´æ¥ä½¿ç”¨ç”Ÿæˆå™¨è¾“å‡º\n",
    "            answer = result['generator']['replies'][0]\n",
    "            retrieved_context_list.append(\"No context available\")\n",
    "\n",
    "        # è§£æä¸å­˜å‚¨ç­”æ¡ˆ\n",
    "        try:\n",
    "            answer_json = json.loads(answer)\n",
    "        except json.JSONDecodeError:\n",
    "            # å…œåº•ï¼šä¿ç•™é¢˜å·ä¸åŸæ–‡ï¼Œé¿å…è¯„ä¼°é˜¶æ®µâ€œæœªæ‰¾åˆ°é—®é¢˜ qX çš„RAGè¾“å‡ºâ€\n",
    "            answer_json = {\n",
    "                \"meta\": {\"id\": app_id, \"url\": app_url},\n",
    "                \"reply\": {\n",
    "                    \"qid\": f\"q{j+1}\",\n",
    "                    \"question\": questions[j],\n",
    "                    \"answer\": {\n",
    "                        \"full_answer\": str(answer),\n",
    "                        \"simple_answer\": \"\",\n",
    "                        \"extended_simple_answer\": {\"comment\": \"\", \"content\": \"\"}\n",
    "                    },\n",
    "                    \"_parsing_note\": \"model_output_not_valid_json_fallback_used\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # åœ¨ä½ è®¾ç½®å®Œå…œåº• answer_json ä¹‹åè°ƒç”¨ï¼š\n",
    "        answer_json = _try_salvage_nested_json(answer_json)\n",
    "\n",
    "        # å®‰å…¨åˆå¹¶æ¥æºç‰‡æ®µï¼ˆå¦‚æœ‰ï¼‰\n",
    "        try:\n",
    "            if source_docs_export:\n",
    "                answer_json.setdefault(\"source_documents\", source_docs_export)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        answers_list.append(answer_json)\n",
    "\n",
    "\n",
    "\n",
    "    # ä¿å­˜ JSON\n",
    "    output_file_path = os.path.join(output_folder, f\"{app_id}.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(answers_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nSaved answers for App ID {app_id} to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f7955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ åˆ›å»ºåŸºäº5ä¸ªéšç§æ”¿ç­–é—®é¢˜çš„ä¸“é—¨RAGè¯„ä¼°Pipeline...\n",
      "ğŸ¯ å¼€å§‹æ‰§è¡Œéšç§æ”¿ç­–RAGè¯„ä¼°...\n",
      "================================================================================\n",
      "ğŸš€ å¼€å§‹éšç§æ”¿ç­–RAGç³»ç»Ÿç»¼åˆè¯„ä¼°\n",
      "================================================================================\n",
      "âœ… åŠ è½½äº† 10 ä¸ªåº”ç”¨çš„groundtruthæ ‡æ³¨\n",
      "ğŸ“Š æ‰¾åˆ° 9 ä¸ªå¯è¯„ä¼°çš„åº”ç”¨\n",
      "ğŸ¯ å°†è¯„ä¼°å‰ 9 ä¸ªåº”ç”¨: [1361356590, 1435692352, 1458846512, 1493155192, 1498229813, 1588978095, 1665348316, 6447095050, 6474216442]\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1361356590\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.11s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:10<00:00,  1.76s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1435692352\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.45s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1458846512\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.09s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:11<00:00,  1.98s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1493155192\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1498229813\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1588978095\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:11<00:00,  1.87s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.57s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1665348316\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:10<00:00,  1.67s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 6447095050\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:06<00:06,  2.10s/it]"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ åŸºäº5ä¸ªéšç§æ”¿ç­–é—®é¢˜çš„ä¸“é—¨RAGè¯„ä¼°Pipeline\n",
    "print(\"ğŸ¯ åˆ›å»ºåŸºäº5ä¸ªéšç§æ”¿ç­–é—®é¢˜çš„ä¸“é—¨RAGè¯„ä¼°Pipeline...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators.document_mrr import DocumentMRREvaluator\n",
    "from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator\n",
    "from haystack.components.evaluators.sas_evaluator import SASEvaluator\n",
    "\n",
    "\n",
    "# å®šä¹‰5ä¸ªé—®é¢˜åŠå…¶å¯¹åº”çš„ground truthæ ¼å¼\n",
    "PRIVACY_POLICY_QUESTIONS = {\n",
    "    'q1': {\n",
    "        'text': \"1. Does the app declare the collection of data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q2': {\n",
    "        'text': \"2. What type of data does it collect?\",\n",
    "        'type': 'open'  # å¼€æ”¾é¢˜ï¼Œgroundtruth ä¸ºè‡ªç”±æ–‡æœ¬\n",
    "    },\n",
    "    'q3': {\n",
    "        'text': \"3. Does the app declare the purpose of data collection and use?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q4': {\n",
    "        'text': \"4. Can you opt out of data collection or delete data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q5': {\n",
    "        'text': \"5. Does the app share data with third parties?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q6': {\n",
    "        'text': \"6. If the app shares data with third parties, what third parties does the app share data with?\",\n",
    "        'type': 'open'  # å¼€æ”¾é¢˜ï¼Œgroundtruth ä¸ºè‡ªç”±æ–‡æœ¬\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_groundtruth_data(groundtruth_path='../groundtruth.json'):\n",
    "    \"\"\"åŠ è½½groundtruthæ•°æ®\"\"\"\n",
    "    with open(groundtruth_path, 'r', encoding='utf-8') as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºæ›´æ˜“ç”¨çš„æ ¼å¼\n",
    "    gt_dict = {}\n",
    "    for item in gt_data:\n",
    "        app_id = item['id']\n",
    "        gt_dict[app_id] = {\n",
    "            'q1': item['q1'],\n",
    "            'q2': item['q2'], \n",
    "            'q3': item['q3'],\n",
    "            'q4': item['q4'],\n",
    "            'q5': item['q5'],\n",
    "            'q6': item['q6']\n",
    "        }\n",
    "    \n",
    "    print(f\"âœ… åŠ è½½äº† {len(gt_dict)} ä¸ªåº”ç”¨çš„groundtruthæ ‡æ³¨\")\n",
    "    return gt_dict\n",
    "\n",
    "def load_rag_output(app_id, outputs_dir='outputs'):\n",
    "    \"\"\"åŠ è½½å•ä¸ªåº”ç”¨çš„RAGè¾“å‡º\"\"\"\n",
    "    output_file = os.path.join(outputs_dir, f\"{app_id}.json\")\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"âš ï¸ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            rag_data = json.load(f)\n",
    "        return rag_data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½RAGè¾“å‡ºå¤±è´¥ {output_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_question_mapping(rag_outputs):\n",
    "    \"\"\"ä»RAGè¾“å‡ºä¸­æå–é—®é¢˜æ˜ å°„ï¼ˆæŒ‰é¢˜å·å‰ç¼€ 1.~7. æ˜ å°„ï¼‰\"\"\"\n",
    "    question_mapping = {}\n",
    "    for i, output in enumerate(rag_outputs):\n",
    "        try:\n",
    "            qtext = output['reply']['question'].strip()\n",
    "            if qtext.startswith(\"1.\"):\n",
    "                question_mapping['q1'] = output\n",
    "            elif qtext.startswith(\"2.\"):\n",
    "                question_mapping['q2'] = output\n",
    "            elif qtext.startswith(\"3.\"):\n",
    "                question_mapping['q3'] = output\n",
    "            elif qtext.startswith(\"4.\"):\n",
    "                question_mapping['q4'] = output\n",
    "            elif qtext.startswith(\"5.\"):\n",
    "                question_mapping['q5'] = output\n",
    "            elif qtext.startswith(\"6.\"):\n",
    "                question_mapping['q6'] = output\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ å¤„ç†ç¬¬{i+1}ä¸ªè¾“å‡ºæ—¶å‡ºé”™: {e}\")\n",
    "            continue\n",
    "    return question_mapping\n",
    "\n",
    "def create_privacy_evaluation_pipeline():\n",
    "    \"\"\"åˆ›å»ºä»…åŒ…å« Faithfulness / SAS / Context Relevance çš„è¯„ä¼° pipeline\"\"\"\n",
    "    eval_pipeline = Pipeline()\n",
    "    eval_pipeline.add_component(\"faithfulness\", FaithfulnessEvaluator())\n",
    "    eval_pipeline.add_component(\"sas_evaluator\", SASEvaluator(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    eval_pipeline.add_component(\"context_relevance\", ContextRelevanceEvaluator())\n",
    "    return eval_pipeline\n",
    "\n",
    "def evaluate_single_app(app_id, gt_dict, outputs_dir='outputs'):\n",
    "    \"\"\"è¯„ä¼°å•ä¸ªåº”ç”¨\"\"\"\n",
    "    print(f\"\\nğŸ“‹ è¯„ä¼°åº”ç”¨ ID: {app_id}\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    gt_answers = gt_dict.get(app_id)\n",
    "    if not gt_answers:\n",
    "        print(f\"âŒ æœªæ‰¾åˆ°åº”ç”¨ {app_id} çš„groundtruth\")\n",
    "        return None\n",
    "    \n",
    "    rag_outputs = load_rag_output(app_id, outputs_dir)\n",
    "    if not rag_outputs:\n",
    "        print(f\"âŒ æœªæ‰¾åˆ°åº”ç”¨ {app_id} çš„RAGè¾“å‡º\")  \n",
    "        return None\n",
    "    \n",
    "    # æå–é—®é¢˜æ˜ å°„\n",
    "    question_mapping = extract_question_mapping(rag_outputs)\n",
    "    print(f\"âœ… æ‰¾åˆ° {len(question_mapping)} ä¸ªé—®é¢˜çš„è¾“å‡º\")\n",
    "    \n",
    "    # è¯„ä¼°ç»“æœ\n",
    "    evaluation_results = {\n",
    "        'app_id': app_id,\n",
    "        'questions': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # é€ä¸ªé—®é¢˜è¯„ä¼°\n",
    "    questions_data = []  # ç”¨äºpipelineè¯„ä¼°\n",
    "    contexts_data = []\n",
    "    predicted_answers = []\n",
    "    ground_truth_answers = []\n",
    "    \n",
    "    bin_total = 0\n",
    "    bin_correct = 0\n",
    "\n",
    "    for q_key in ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']:\n",
    "        qconf = PRIVACY_POLICY_QUESTIONS[q_key]\n",
    "        qtype = qconf.get('type', 'binary')\n",
    "\n",
    "        if q_key in question_mapping:\n",
    "            rag_output = question_mapping[q_key]\n",
    "            try:\n",
    "                predicted_simple = rag_output['reply']['answer'].get('simple_answer', '')\n",
    "                predicted_full = rag_output['reply']['answer'].get('full_answer', '')\n",
    "                reference = rag_output['reply'].get('reference', '')\n",
    "\n",
    "                # ground truth\n",
    "                if qtype == 'binary':\n",
    "                    gt_label = (gt_answers[q_key] or '').strip().lower()  # 'y'/'n'\n",
    "                    gt_answer = qconf['expected_simple_answers'][gt_label]\n",
    "                    pred_norm = 'y' if predicted_simple.lower().startswith('y') else 'n'\n",
    "                    correct = (pred_norm == gt_label)\n",
    "                    bin_total += 1\n",
    "                    bin_correct += 1 if correct else 0\n",
    "                else:\n",
    "                    # å¼€æ”¾é¢˜ç›´æ¥ç”¨æ–‡æœ¬ groundtruth\n",
    "                    gt_answer = gt_answers[q_key]\n",
    "                    pred_norm = None\n",
    "                    correct = None  # å¼€æ”¾é¢˜ä¸å‚ä¸äºŒåˆ†ç±»å‡†ç¡®ç‡\n",
    "\n",
    "                evaluation_results['questions'][q_key] = {\n",
    "                    'question_text': qconf['text'],\n",
    "                    'type': qtype,\n",
    "                    'ground_truth_answer': gt_answer,\n",
    "                    'predicted_simple': predicted_simple,\n",
    "                    'predicted_full': predicted_full,\n",
    "                    'predicted_normalized': pred_norm,\n",
    "                    'correct': correct,\n",
    "                    'reference': reference\n",
    "                }\n",
    "\n",
    "                # è¯„ä¼°è¾“å…¥ï¼ˆSAS/Faithfulness/Context Relevanceï¼‰\n",
    "                questions_data.append(qconf['text'])\n",
    "                # ä¼˜å…ˆä½¿ç”¨ top-level æˆ– reply ä¸­çš„ source_documents çš„ excerpt/content ä½œä¸º context\n",
    "                src_docs = rag_output.get('source_documents') or rag_output.get('reply', {}).get('source_documents') or []\n",
    "                if src_docs:\n",
    "                    # æ‹¼æ¥å¤šæ®µä»¥æé«˜ä¸Šä¸‹æ–‡è¦†ç›–\n",
    "                    excerpts = []\n",
    "                    for sd in src_docs:\n",
    "                        ex = sd.get('excerpt') or sd.get('content') or sd.get('text') or ''\n",
    "                        if ex:\n",
    "                            excerpts.append(ex)\n",
    "                    context_text = '\\n\\n'.join(excerpts) if excerpts else (reference or 'No relevant content found')\n",
    "                    contexts_data.append([context_text])\n",
    "                else:\n",
    "                    contexts_data.append([reference] if reference else ['No relevant content found'])\n",
    "\n",
    "                # åˆ†å¼€ä¸º Faithfulness ä¸ SAS å‡†å¤‡é¢„æµ‹ç­”æ¡ˆï¼š\n",
    "                # - faithfulness ä½¿ç”¨ full_answerï¼ˆæ›´æ˜“åˆ¤æ–­è¯æ®æ”¯æŒï¼‰\n",
    "                # - sas ä½¿ç”¨ simple_answer å¯¹äº binaryï¼Œå¼€æ”¾é¢˜ä»ç”¨ full_answer\n",
    "                if 'predicted_answers_faithfulness' not in locals():\n",
    "                    predicted_answers_faithfulness = []\n",
    "                if 'predicted_answers_sas' not in locals():\n",
    "                    predicted_answers_sas = []\n",
    "\n",
    "                predicted_answers_faithfulness.append(predicted_full)\n",
    "                if qtype == 'binary':\n",
    "                    predicted_answers_sas.append(predicted_simple)\n",
    "                else:\n",
    "                    predicted_answers_sas.append(predicted_full)\n",
    "\n",
    "                ground_truth_answers.append(gt_answer)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ å¤„ç†é—®é¢˜ {q_key} æ—¶å‡ºé”™: {e}\")\n",
    "                evaluation_results['questions'][q_key] = {\n",
    "                    'question_text': qconf['text'],\n",
    "                    'type': qtype,\n",
    "                    'ground_truth_answer': gt_answers.get(q_key, ''),\n",
    "                    'error': str(e),\n",
    "                    'correct': False if qtype == 'binary' else None\n",
    "                }\n",
    "        else:\n",
    "            print(f\"âš ï¸ æœªæ‰¾åˆ°é—®é¢˜ {q_key} çš„RAGè¾“å‡º\")\n",
    "            evaluation_results['questions'][q_key] = {\n",
    "                'question_text': qconf['text'],\n",
    "                'type': qtype,\n",
    "                'missing': True,\n",
    "                'correct': False if qtype == 'binary' else None\n",
    "            }\n",
    "\n",
    "    # å‡†ç¡®ç‡ï¼ˆä»…äºŒåˆ†ç±»ï¼‰\n",
    "    accuracy = (bin_correct / bin_total) if bin_total > 0 else 0.0\n",
    "    evaluation_results['summary'] = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct_count': bin_correct,\n",
    "        'total_count': bin_total,\n",
    "        'questions_found': len(question_mapping)\n",
    "    }\n",
    "\n",
    "    predicted_answers_faithfulness = locals().get('predicted_answers_faithfulness', [])\n",
    "    predicted_answers_sas = locals().get('predicted_answers_sas', [])\n",
    "    # å…¼å®¹æ—§ä»£ç ï¼špredicted_answers ä¿æŒéç©ºè¡¨ç¤ºå¯ä»¥è¿è¡Œè¯„ä¼°\n",
    "    predicted_answers = predicted_answers_faithfulness or predicted_answers_sas or []\n",
    "    \n",
    "    # è¿è¡ŒHaystackè¯„ä¼°pipelineï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰\n",
    "    if questions_data and predicted_answers:\n",
    "        try:\n",
    "            print(\"ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\")\n",
    "            eval_pipeline = create_privacy_evaluation_pipeline()\n",
    "            \n",
    "            # å‡†å¤‡æ–‡æ¡£æ•°æ®ï¼ˆä»referenceä¸­æå–ï¼‰\n",
    "            retrieved_docs = []\n",
    "            ground_truth_docs = []\n",
    "            \n",
    "            for i, context_list in enumerate(contexts_data):\n",
    "                from haystack import Document\n",
    "                \n",
    "                # åˆ›å»ºæ£€ç´¢æ–‡æ¡£\n",
    "                if context_list and context_list[0] != 'No context available':\n",
    "                    doc = Document(content=context_list[0], meta={\"source\": \"privacy_policy\"})\n",
    "                    retrieved_docs.append([doc])\n",
    "                    ground_truth_docs.append(doc)\n",
    "                else:\n",
    "                    empty_doc = Document(content=\"No relevant content found\", meta={\"source\": \"empty\"})\n",
    "                    retrieved_docs.append([empty_doc])\n",
    "                    ground_truth_docs.append(empty_doc)\n",
    "            \n",
    "            # è¿è¡Œè¯„ä¼°\n",
    "            haystack_results = eval_pipeline.run({\n",
    "                \"faithfulness\": {\n",
    "                    \"questions\": questions_data,\n",
    "                    \"contexts\": contexts_data,\n",
    "                    \"predicted_answers\": predicted_answers_faithfulness,\n",
    "                },\n",
    "                \"sas_evaluator\": {\n",
    "                    \"predicted_answers\": predicted_answers_sas,\n",
    "                    \"ground_truth_answers\": ground_truth_answers\n",
    "                },\n",
    "                \"context_relevance\": {\n",
    "                    \"questions\": questions_data,\n",
    "                    \"contexts\": contexts_data\n",
    "                },\n",
    "            })\n",
    "            \n",
    "            # æ·»åŠ Haystackè¯„ä¼°ç»“æœ\n",
    "            evaluation_results['haystack_metrics'] = {\n",
    "                'faithfulness': {\n",
    "                    'individual_scores': haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\") else 0\n",
    "                },\n",
    "                'sas': {\n",
    "                    'individual_scores': haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\") else 0\n",
    "                },\n",
    "                'context_relevance': {\n",
    "                    'individual_scores': haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\") else 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(\"âœ… Haystackè¯„ä¼°å®Œæˆ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Haystackè¯„ä¼°å¤±è´¥: {e}\")\n",
    "            evaluation_results['haystack_metrics'] = {'error': str(e)}\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def run_comprehensive_privacy_evaluation():\n",
    "    \"\"\"è¿è¡Œå®Œæ•´çš„éšç§æ”¿ç­–è¯„ä¼°\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ å¼€å§‹éšç§æ”¿ç­–RAGç³»ç»Ÿç»¼åˆè¯„ä¼°\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # åŠ è½½groundtruthæ•°æ®\n",
    "    gt_dict = load_groundtruth_data()\n",
    "    \n",
    "    # è·å–æ‰€æœ‰æœ‰è¾“å‡ºæ–‡ä»¶çš„åº”ç”¨ID\n",
    "    outputs_dir = 'outputs'\n",
    "    available_apps = []\n",
    "    \n",
    "    if os.path.exists(outputs_dir):\n",
    "        for filename in os.listdir(outputs_dir):\n",
    "            if filename.endswith('.json') and filename.replace('.json', '').isdigit():\n",
    "                app_id = int(filename.replace('.json', ''))\n",
    "                if app_id in gt_dict:\n",
    "                    available_apps.append(app_id)\n",
    "    \n",
    "    print(f\"ğŸ“Š æ‰¾åˆ° {len(available_apps)} ä¸ªå¯è¯„ä¼°çš„åº”ç”¨\")\n",
    "    \n",
    "    if not available_apps:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å¯è¯„ä¼°çš„åº”ç”¨ï¼\")\n",
    "        return\n",
    "    \n",
    "    # é€‰æ‹©è¦è¯„ä¼°çš„åº”ç”¨ï¼ˆè¿™é‡Œè¯„ä¼°å‰3ä¸ªä½œä¸ºç¤ºä¾‹ï¼‰\n",
    "    apps_to_evaluate = available_apps  # å¯ä»¥ä¿®æ”¹æ•°é‡\n",
    "    print(f\"ğŸ¯ å°†è¯„ä¼°å‰ {len(apps_to_evaluate)} ä¸ªåº”ç”¨: {apps_to_evaluate}\")\n",
    "    \n",
    "    # é€ä¸ªè¯„ä¼°åº”ç”¨\n",
    "    all_results = []\n",
    "    overall_stats = {\n",
    "        'total_apps': 0,\n",
    "        'total_questions': 0,\n",
    "        'total_correct': 0,\n",
    "        'per_question_stats': {q: {'correct': 0, 'total': 0} for q in ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']},\n",
    "        'haystack_aggregated': {\n",
    "            'faithfulness_scores': [],\n",
    "            'sas_scores': [],\n",
    "            'context_relevance_scores': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for app_id in apps_to_evaluate:\n",
    "        result = evaluate_single_app(app_id, gt_dict, outputs_dir)\n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # æ›´æ–°æ€»ä½“ç»Ÿè®¡\n",
    "            overall_stats['total_apps'] += 1\n",
    "            overall_stats['total_questions'] += result['summary']['total_count']\n",
    "            overall_stats['total_correct'] += result['summary']['correct_count']\n",
    "            \n",
    "            # æ›´æ–°å„é—®é¢˜ç»Ÿè®¡\n",
    "            for q_key, q_data in result['questions'].items():\n",
    "                overall_stats['per_question_stats'][q_key]['total'] += 1\n",
    "                if q_data.get('correct', False):\n",
    "                    overall_stats['per_question_stats'][q_key]['correct'] += 1\n",
    "            \n",
    "            # èšåˆHaystackè¯„ä¼°ç»“æœ\n",
    "            if 'haystack_metrics' in result and 'error' not in result['haystack_metrics']:\n",
    "                hm = result['haystack_metrics']\n",
    "                if hm.get('faithfulness', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['faithfulness_scores'].extend(\n",
    "                        hm['faithfulness']['individual_scores']\n",
    "                    )\n",
    "                if hm.get('sas', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['sas_scores'].extend(\n",
    "                        hm['sas']['individual_scores']\n",
    "                    )\n",
    "                if hm.get('context_relevance', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['context_relevance_scores'].extend(\n",
    "                        hm['context_relevance']['individual_scores']\n",
    "                    )\n",
    "    \n",
    "    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡\n",
    "    final_results = {\n",
    "        'metadata': {\n",
    "            'evaluation_date': str(pd.Timestamp.now()),\n",
    "            'total_apps_evaluated': overall_stats['total_apps'],\n",
    "            'total_questions_evaluated': overall_stats['total_questions'],\n",
    "            'questions_definition': PRIVACY_POLICY_QUESTIONS\n",
    "        },\n",
    "        'classification_metrics': {\n",
    "            'overall_accuracy': overall_stats['total_correct'] / overall_stats['total_questions'] if overall_stats['total_questions'] > 0 else 0,\n",
    "            'per_question_accuracy': {\n",
    "                q: stats['correct'] / stats['total'] if stats['total'] > 0 else 0 \n",
    "                for q, stats in overall_stats['per_question_stats'].items()\n",
    "            }\n",
    "        },\n",
    "        'haystack_metrics': {\n",
    "            'faithfulness_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['faithfulness_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['faithfulness_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['faithfulness_scores'] else 0,\n",
    "            'sas_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['sas_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['sas_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['sas_scores'] else 0,\n",
    "            'context_relevance_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['context_relevance_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['context_relevance_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['context_relevance_scores'] else 0\n",
    "        },\n",
    "        'detailed_results': all_results\n",
    "    }\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š éšç§æ”¿ç­–RAGè¯„ä¼°ç»“æœ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ æ€»ä½“å‡†ç¡®ç‡: {final_results['classification_metrics']['overall_accuracy']:.3f}\")\n",
    "    print(f\"ğŸ“Š è¯„ä¼°äº† {final_results['metadata']['total_apps_evaluated']} ä¸ªåº”ç”¨ï¼Œ{final_results['metadata']['total_questions_evaluated']} ä¸ªé—®é¢˜\")\n",
    "    \n",
    "    print(\"\\nğŸ” å„é—®é¢˜å‡†ç¡®ç‡:\")\n",
    "    for q_key, accuracy in final_results['classification_metrics']['per_question_accuracy'].items():\n",
    "        q_text = PRIVACY_POLICY_QUESTIONS[q_key]['text'][:50] + \"...\"\n",
    "        print(f\"  {q_key}: {accuracy:.3f} - {q_text}\")\n",
    "    \n",
    "    if final_results['haystack_metrics']['faithfulness_average'] > 0:\n",
    "        print(f\"\\nğŸ¯ Haystackè¯„ä¼°æŒ‡æ ‡:\")\n",
    "        print(f\"  ğŸ” Faithfulness: {final_results['haystack_metrics']['faithfulness_average']:.3f}\")\n",
    "        print(f\"  ğŸ“ SAS: {final_results['haystack_metrics']['sas_average']:.3f}\")\n",
    "        print(f\"  ğŸ“ Context Relevance: {final_results['haystack_metrics']['context_relevance_average']:.3f}\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    output_path = 'eval/privacy_policy_rag_evaluation.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# æ‰§è¡Œè¯„ä¼°\n",
    "print(\"ğŸ¯ å¼€å§‹æ‰§è¡Œéšç§æ”¿ç­–RAGè¯„ä¼°...\")\n",
    "evaluation_results = run_comprehensive_privacy_evaluation()\n",
    "\n",
    "print(\"\\nğŸ‰ éšç§æ”¿ç­–RAGè¯„ä¼°å®Œæˆï¼\")\n",
    "print(\"ğŸ“‹ è¯„ä¼°æ¶µç›–:\")\n",
    "print(\"   âœ… 5ä¸ªéšç§æ”¿ç­–æ ¸å¿ƒé—®é¢˜çš„åˆ†ç±»å‡†ç¡®ç‡\")\n",
    "print(\"   ğŸ” Haystackä¸“ä¸šè¯„ä¼°æŒ‡æ ‡ (Faithfulness, SAS, Context Relevance)\")\n",
    "print(\"   ğŸ“Š è¯¦ç»†çš„é€åº”ç”¨ã€é€é—®é¢˜åˆ†æ\")\n",
    "print(\"   ğŸ’¾ å®Œæ•´ç»“æœä¿å­˜ä¸ºJSONæ ¼å¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
