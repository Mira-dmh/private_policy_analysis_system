{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c1ad72",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Incremental Indexing\n",
    "This notebook demonstrates how to combine indexing and RAG pipelines to process URLs one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829a8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in c:\\users\\xdn13\\appdata\\roaming\\python\\python311\\site-packages (2.18.0)\n",
      "Requirement already satisfied: docstring-parser in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.17.0)\n",
      "Requirement already satisfied: filetype in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: haystack-experimental in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.13.0)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.25.1)\n",
      "Requirement already satisfied: lazy-imports in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.0.1)\n",
      "Requirement already satisfied: more-itertools in d:\\python3.11\\lib\\site-packages (from haystack-ai) (10.8.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.5)\n",
      "Requirement already satisfied: numpy in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.3.3)\n",
      "Requirement already satisfied: openai>=1.56.1 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.107.2)\n",
      "Requirement already satisfied: posthog!=3.12.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.7.4)\n",
      "Requirement already satisfied: pydantic in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.11.7)\n",
      "Requirement already satisfied: python-dateutil in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (9.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\python3.11\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.56.1->haystack-ai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\python3.11\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (2.4.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->haystack-ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.25.1)\n",
      "Requirement already satisfied: datasets>=3.6.0 in d:\\python3.11\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.33.0)\n",
      "Requirement already satisfied: packaging in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.12.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\python3.11\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets>=3.6.0) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.6.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.6.0) (1.17.0)\n",
      "Requirement already satisfied: sentence-transformers>=4.1.0 in d:\\python3.11\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.52.4)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.7.0)\n",
      "Requirement already satisfied: scipy in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (0.33.0)\n",
      "Requirement already satisfied: Pillow in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=4.1.0) (2025.3.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=4.1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->sentence-transformers>=4.1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=4.1.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (3.6.0)\n",
      "Requirement already satisfied: accelerate in d:\\python3.11\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\python3.11\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "!pip install haystack-ai\n",
    "!pip install \"datasets>=3.6.0\"\n",
    "!pip install \"sentence-transformers>=4.1.0\"\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.components.embedders.cohere import CohereDocumentEmbedder, CohereTextEmbedder\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator,FaithfulnessEvaluator\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack_integrations.components.rankers.cohere import CohereRanker\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 从.env文件读取API key\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = cohere_api_key if cohere_api_key else \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key if openai_api_key else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a98202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取本地 index_table.json 文件\n",
    "with open(\"../files/index_table.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0e93",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d5e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a privacy policy expert. You are provided with {{app_url}}, which contains the privacy policy document for an app.\n",
    "Your task is to:\n",
    " - answer the question based on the privacy policy document,\n",
    " - provide references for your answers based on the section in the privacy policy document from which your answer is generated,\n",
    " - produce your results strictly in the JSON format below (no extra text beyond JSON),\n",
    " - ensure that the 'url' in the 'meta' section is exactly {{app_url}}.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "   \"meta\": {\n",
    "       \"id\": {{ app_id }},\n",
    "       \"url\": {{ app_url }},\n",
    "       \"title\": {{ app_name | tojson }}\n",
    "   },\n",
    "   \"reply\": {\n",
    "       \"qid\": \"{{ qid }}\",\n",
    "       \"question\": \"{{ question | escape }}\",\n",
    "       \"answer\": {\n",
    "           \"full_answer\": \"{{ full_answer | escape }}\",\n",
    "           \"simple_answer\": \"{{ simple_answer | escape }}\",\n",
    "           \"extended_simple_answer\": {\n",
    "               \"comment\": \"{{ extended_comment | escape }}\",\n",
    "               \"content\": \"{{ extended_content | escape }}\"\n",
    "           }\n",
    "       },\n",
    "       \"analysis\": \"{{ analysis | escape }}\",\n",
    "       \"reference\": \"{{ reference | escape }}\"\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Approach each question systematically:\n",
    "   a. Understand the question: Break down the question into specific components or sub-questions if needed.\n",
    "\n",
    "   b. Identify relevant context from the privacy policy.\n",
    "   c. Analyze the context and link it back to the question.\n",
    "   d. Formulate the answer for each JSON field.\n",
    "   e. Provide references:  (original text + 'URL: {{app_url}}'). If none, report 'N/A. URL: {{app_url}}'. Note that the original text must come from only the relevant context in page {{app_url}}\n",
    "\n",
    "2. Output structure:\n",
    "   - full_answer: must integrate info from both simple_answer and extended_simple_answer. The full answer section must not be empty.\n",
    "   - simple_answer: follow the rules above. This section must not be empty.\n",
    "   - extended_simple_answer: follow the rules above, or empty if not specified\n",
    "   - analysis: describe your reasoning\n",
    "   - reference: original text snippets + URL. The context must come from the app URL {{app_url}}. Attach the {{app_url}} in the end. Ensure JSON compatibility by replacing double quotes with single quotes.\n",
    "\n",
    "\n",
    "3. Special rules for the `simple_answer` and `extended_simple_answer` fields:\n",
    "   - If the question is “1. Does the app declare the collection of data?”:\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is “2. If the app declares the collection of data, what type of data does it collect?':\n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer: \n",
    "           - comment: \"data collected\"\n",
    "           - content: list of data types collected\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "   - If the question is \"3. Does the app declare the purpose of data collection and use?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"4. Can the user opt out of data collection or delete data?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"5. Does the app share data with third parties?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"6. If the app shares data with third parties, what third parties does the app share data with?\": \n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer:\n",
    "           - comment: \"third parties\"\n",
    "           - content: list of third parties\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "  URL: {{ doc.meta['url'] }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'outputs/'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436c1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xdn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize document store and pipelines\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# ===================== 静态无状态组件（循环外建一次） =====================\n",
    "# 为防止部分站点拒绝默认UA，添加浏览器UA，并在失败时不抛出异常\n",
    "fetcher = LinkContentFetcher(raise_on_failure=False)\n",
    "converter = HTMLToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "\n",
    "splitter = DocumentSplitter(\n",
    "    split_by=\"word\",\n",
    "    split_length=220,\n",
    "    split_overlap=50\n",
    ")\n",
    "\n",
    "embedder = CohereDocumentEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "query_embedder = CohereTextEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "\n",
    "prompt_builder = PromptBuilder(\n",
    "    template=prompt,\n",
    "    required_variables=[\"query\", \"app_id\", \"app_url\", \"question\", \"documents\"]\n",
    ")\n",
    "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
    "answer_builder = AnswerBuilder()\n",
    "\n",
    "\n",
    "reranker = CohereRanker(model=\"rerank-english-v3.0\", top_k=5)\n",
    "\n",
    "\n",
    "# ===================== 占位的可变组件（先绑定一个临时 store） =====================\n",
    "# 关键：writer / retriever 的实例是“可变”的，我们每轮替换它们的 document_store 即可\n",
    "_initial_store = InMemoryDocumentStore()\n",
    "writer = DocumentWriter(document_store=_initial_store, policy=DuplicatePolicy.OVERWRITE)\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=_initial_store, top_k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d99054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001A86182F450>\n",
       "🚅 Components\n",
       "  - query_embedder: CohereTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - reranker: CohereRanker\n",
       "  - prompt: PromptBuilder\n",
       "  - generator: OpenAIGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "🛤️ Connections\n",
       "  - query_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> reranker.documents (list[Document])\n",
       "  - reranker.documents -> prompt.documents (List[Document])\n",
       "  - reranker.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt.prompt -> generator.prompt (str)\n",
       "  - generator.replies -> answer_builder.replies (list[str])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================== Pipeline 只构建一次（无 None） =====================\n",
    "# 索引管道：fetch -> convert -> clean -> split -> embed -> write\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"fetcher\", fetcher)\n",
    "indexing.add_component(\"converter\", converter)\n",
    "indexing.add_component(\"cleaner\", cleaner)\n",
    "indexing.add_component(\"splitter\", splitter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)  # 注意：是真实 writer，不是 None\n",
    "\n",
    "indexing.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "indexing.connect(\"converter\", \"cleaner\")\n",
    "indexing.connect(\"cleaner\", \"splitter\")\n",
    "indexing.connect(\"splitter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")\n",
    "\n",
    "# RAG 管道：query_embedder -> retriever -> prompt -> generator -> answer_builder\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"query_embedder\", query_embedder)\n",
    "rag.add_component(\"retriever\", retriever)  # 注意：是真实 retriever，不是 None\n",
    "rag.add_component(\"reranker\", reranker)    # ✅ 新增重排器\n",
    "rag.add_component(\"prompt\", prompt_builder)\n",
    "rag.add_component(\"generator\", generator)\n",
    "rag.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# ✅ 先召回 → 再重排 → 再送给 prompt / answer_builder\n",
    "rag.connect(\"retriever.documents\", \"reranker.documents\")\n",
    "rag.connect(\"reranker.documents\", \"prompt.documents\")\n",
    "rag.connect(\"reranker.documents\", \"answer_builder.documents\")\n",
    "rag.connect(\"prompt\", \"generator\")\n",
    "rag.connect(\"generator.replies\", \"answer_builder.replies\")\n",
    "# （不要连 query_embedder.text -> answer_builder.query）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c621",
   "metadata": {},
   "source": [
    "## Use LLM generate replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c9ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"1. Does the app declare the collection of data?\",\n",
    "    \"2. If the app declares the collection of data, what type of data does it collect?\",\n",
    "    \"3. Does the app declare the purpose of data collection and use?\",\n",
    "    \"4. Can the user opt out of data collection or delete data?\",\n",
    "    \"5. Does the app share data with third parties?\",\n",
    "    \"6. If the app shares data with third parties, what third parties does the app share data with?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ===================== 检索短查询映射（Step B 新增） =====================\n",
    "# 说明：这些是“给向量检索用的短语”，尽量去掉模板词，只保留语义核心，暂时不用\n",
    "query_map = {\n",
    "    \"1. Does the app declare the collection of data?\": \"data collection\",\n",
    "    \"2. What type of data does it collect?\": \"type of data collected\",\n",
    "    \"3. Does the app declare the purpose of data collection and use?\": \"purpose of data collection\",\n",
    "    \"4. Can you opt out of data collection or delete data?\": \"opt out of data collection\",\n",
    "    \"5. Does the app share data with third parties?\": \"data sharing with third parties\",\n",
    "    \"6. If the app shares data with third parties, what third parties does the app share data with?\": \"which third parties receive data\",\n",
    "}\n",
    "\n",
    "# 定义输出文件夹\n",
    "output_folder = 'outputs/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# 让用户可自定义选取数量\n",
    "num_to_process = 1  # 修改此处即可设定处理前几个URL\n",
    "\n",
    "# 文档摘要长度设定\n",
    "MAX_DOC_EXCERPT = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12d7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 兜底对象 answer_json 构造完之后，立刻加上这段“二次修复”\n",
    "import re, json\n",
    "\n",
    "def _try_salvage_nested_json(answer_json):\n",
    "    # 仅在 full_answer 是字符串、且像 JSON 时尝试\n",
    "    raw = (answer_json.get(\"reply\", {}).get(\"answer\", {}).get(\"full_answer\") or \"\").strip()\n",
    "    if not (isinstance(raw, str) and raw.startswith(\"{\")):\n",
    "        return answer_json\n",
    "\n",
    "    # 1) 去掉常见的“结尾多逗号”\n",
    "    fixed = re.sub(r\",\\s*([}\\]])\", r\"\\1\", raw)\n",
    "    # 2) 去掉围栏 ```json ... ```\n",
    "    fixed = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", fixed)\n",
    "\n",
    "    try:\n",
    "        cand = json.loads(fixed)\n",
    "        # 若 cand 本身就是我们期待的结构（含 reply/answer），直接用它替换\n",
    "        if isinstance(cand, dict) and \"reply\" in cand and \"answer\" in cand.get(\"reply\", {}):\n",
    "            # 保留现有 qid，避免丢题号\n",
    "            qid = answer_json.get(\"reply\", {}).get(\"qid\")\n",
    "            cand.setdefault(\"reply\", {}).setdefault(\"qid\", qid)\n",
    "            return cand\n",
    "    except Exception:\n",
    "        pass\n",
    "    return answer_json\n",
    "\n",
    "\n",
    "# 生成候选抓取URL（http/https、末尾斜杠、常见隐私路径）\n",
    "def _generate_candidate_urls(url: str):\n",
    "     candidates = []\n",
    "     try:\n",
    "         parsed = urlparse(url)\n",
    "         base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "         # 原始\n",
    "         candidates.append(url)\n",
    "         # 末尾斜杠变体\n",
    "         if not url.endswith(\"/\"):\n",
    "             candidates.append(url + \"/\")\n",
    "         # http 变体\n",
    "         if parsed.scheme.lower() == \"https\":\n",
    "             candidates.append(url.replace(\"https://\", \"http://\", 1))\n",
    "             if not url.endswith(\"/\"):\n",
    "                 candidates.append((url + \"/\").replace(\"https://\", \"http://\", 1))\n",
    "         # 常见隐私路径（如果给的是站点首页或错误路径）\n",
    "         common_paths = [\n",
    "             \"/privacy\", \"/privacy/\", \"/privacy-policy\", \"/privacy-policy/\",\n",
    "             \"/privacy-policy.html\", \"/privacypolicy\", \"/privacypolicy/\",\n",
    "             \"/privacypolicy.html\"\n",
    "         ]\n",
    "         for p in common_paths:\n",
    "             candidates.append(base + p)\n",
    "     except Exception:\n",
    "         pass\n",
    "     # 去重保序\n",
    "     seen = set()\n",
    "     uniq = []\n",
    "     for u in candidates:\n",
    "         if u and u not in seen:\n",
    "             uniq.append(u)\n",
    "             seen.add(u)\n",
    "     return uniq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdecb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Document 1/1: None (1361356590)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "|Processing Questions for None:  17%|█▋        | 1/6 [00:04<00:21,  4.27s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        1. Does the app declare the collection of data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  33%|███▎      | 2/6 [00:07<00:14,  3.62s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        2. If the app declares the collection of data, what type of data does it collect?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  50%|█████     | 3/6 [00:10<00:10,  3.34s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        3. Does the app declare the purpose of data collection and use?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  67%|██████▋   | 4/6 [00:14<00:06,  3.44s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        4. Can the user opt out of data collection or delete data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  83%|████████▎ | 5/6 [00:18<00:03,  3.78s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        5. Does the app share data with third parties?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None: 100%|██████████| 6/6 [00:21<00:00,  3.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        6. If the app shares data with third parties, what third parties does the app share data with?\n",
      "        \n",
      "\n",
      "Saved answers for App ID 1361356590 to outputs/1361356590.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== 主循环：每个 App 独立 store，但 pipeline 不重建 =====================\n",
    "for idx, item in enumerate(data[:num_to_process]):\n",
    "    app_id = item.get('id')\n",
    "    app_name = item.get('title')\n",
    "    app_url = item.get('url')\n",
    "    if not app_url:\n",
    "        print(f\"No URL found for App ID: {app_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing Document {idx + 1}/{num_to_process}: {app_name} ({app_id})\")\n",
    "\n",
    "    # —— 核心：为当前 App 创建独立的 store，并“就地替换” writer/retriever 所引用的 store —— #\n",
    "    current_store = InMemoryDocumentStore()\n",
    "    writer.document_store = current_store          # 索引写入到当前 store\n",
    "    retriever.document_store = current_store       # 检索从当前 store 读取\n",
    "\n",
    "\n",
    "    # 索引当前 app 的页面（含多URL回退，避免 400/403/404 直接失败）\n",
    "    success = False\n",
    "    for try_url in _generate_candidate_urls(app_url):\n",
    "        try:\n",
    "            _ = indexing.run({\"fetcher\": {\"urls\": [try_url]}})\n",
    "            # 若写入成功应有文档\n",
    "            if getattr(current_store, \"count_documents\", None):\n",
    "                if current_store.count_documents() > 0:\n",
    "                    success = True\n",
    "                    if try_url != app_url:\n",
    "                        print(f\"[indexing] Fallback URL used: {try_url}\")\n",
    "                    break\n",
    "            else:\n",
    "                # 不支持计数则以不报错视为成功\n",
    "                success = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[indexing] Failed on {try_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not success:\n",
    "        print(f\"⚠️ Unable to fetch any content for App ID {app_id}. Skipping this app.\")\n",
    "        continue\n",
    "\n",
    "    answers_list = []\n",
    "\n",
    "    for j in tqdm(range(len(questions)), desc=f\"|Processing Questions for {app_name}\", unit=\"question\"):\n",
    "        query = f\"\"\"\n",
    "        You are analyzing the '{app_name}' with URL: {app_url}.\n",
    "        Answer the following questions based on the privacy policy document:\n",
    "        {questions[j]}\n",
    "        \"\"\"\n",
    "        # Step B: 为检索构造“短查询” —— 更聚焦、更稳健\n",
    "        retriever_query = query_map.get(j, questions[j])\n",
    "\n",
    "        result = rag.run({\n",
    "            # ✅ 用短查询进行向量召回\n",
    "            \"query_embedder\": {\"text\": questions[j]},\n",
    "            # ✅ 放大召回池；重排器会压到 top_k=5（见上面的 reranker.top_k）\n",
    "            \"retriever\": {\"top_k\": 15},\n",
    "            \"reranker\": {\"query\": questions[j]},  # ✅ 为 CohereRanker 提供必需的 query\n",
    "            \"prompt\": {\"qid\": f\"q{j+1}\", \"query\": query, \"app_id\": app_id, \"app_url\": app_url, \"question\": questions[j]},\n",
    "            \"answer_builder\": {\"query\": query}\n",
    "        })\n",
    "\n",
    "        generated_answers = result['answer_builder']['answers']\n",
    "\n",
    "        source_docs_export = []\n",
    "        retrieved_context_list = []\n",
    "\n",
    "        if generated_answers:\n",
    "            structured_answer = generated_answers[0]\n",
    "            answer = structured_answer.data\n",
    "            source_documents = structured_answer.documents\n",
    "\n",
    "            print(f\"Question {j+1} answered using {len(source_documents)} document (forced single)\")\n",
    "            print(f\"Query: {structured_answer.query}\")\n",
    "\n",
    "            for d in source_documents:\n",
    "                excerpt = (d.content or \"\")[:500]\n",
    "                if len(d.content or \"\") > 500: excerpt += \"...\"\n",
    "                source_docs_export.append({\n",
    "                    \"id\": getattr(d, 'id', None),\n",
    "                    \"score\": getattr(d, 'score', None),\n",
    "                    \"excerpt\": excerpt,\n",
    "                    \"url\": (getattr(d, 'meta', {}) or {}).get('url')\n",
    "                })\n",
    "                retrieved_context_list.append(excerpt)\n",
    "        else:\n",
    "            # 回退到直接使用生成器输出\n",
    "            answer = result['generator']['replies'][0]\n",
    "            retrieved_context_list.append(\"No context available\")\n",
    "\n",
    "        # 解析与存储答案\n",
    "        try:\n",
    "            answer_json = json.loads(answer)\n",
    "        except json.JSONDecodeError:\n",
    "            # 兜底：保留题号与原文，避免评估阶段“未找到问题 qX 的RAG输出”\n",
    "            answer_json = {\n",
    "                \"meta\": {\"id\": app_id, \"url\": app_url},\n",
    "                \"reply\": {\n",
    "                    \"qid\": f\"q{j+1}\",\n",
    "                    \"question\": questions[j],\n",
    "                    \"answer\": {\n",
    "                        \"full_answer\": str(answer),\n",
    "                        \"simple_answer\": \"\",\n",
    "                        \"extended_simple_answer\": {\"comment\": \"\", \"content\": \"\"}\n",
    "                    },\n",
    "                    \"_parsing_note\": \"model_output_not_valid_json_fallback_used\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # 在你设置完兜底 answer_json 之后调用：\n",
    "        answer_json = _try_salvage_nested_json(answer_json)\n",
    "\n",
    "        # 安全合并来源片段（如有）\n",
    "        try:\n",
    "            if source_docs_export:\n",
    "                answer_json.setdefault(\"source_documents\", source_docs_export)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        answers_list.append(answer_json)\n",
    "\n",
    "\n",
    "\n",
    "    # 保存 JSON\n",
    "    output_file_path = os.path.join(output_folder, f\"{app_id}.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(answers_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nSaved answers for App ID {app_id} to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f7955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 创建基于5个隐私政策问题的专门RAG评估Pipeline...\n",
      "🎯 开始执行隐私政策RAG评估...\n",
      "================================================================================\n",
      "🚀 开始隐私政策RAG系统综合评估\n",
      "================================================================================\n",
      "✅ 加载了 10 个应用的groundtruth标注\n",
      "📊 找到 9 个可评估的应用\n",
      "🎯 将评估前 9 个应用: [1361356590, 1435692352, 1458846512, 1493155192, 1498229813, 1588978095, 1665348316, 6447095050, 6474216442]\n",
      "\n",
      "📋 评估应用 ID: 1361356590\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.11s/it]\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.76s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1435692352\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.33s/it]\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.45s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1458846512\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.09s/it]\n",
      "100%|██████████| 6/6 [00:11<00:00,  1.98s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1493155192\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1498229813\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.35s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1588978095\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:11<00:00,  1.87s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.57s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 1665348316\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.55s/it]\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.67s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Haystack评估完成\n",
      "\n",
      "📋 评估应用 ID: 6447095050\n",
      "✅ 找到 6 个问题的输出\n",
      "🎯 运行Haystack评估pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      " 50%|█████     | 3/6 [00:06<00:06,  2.10s/it]"
     ]
    }
   ],
   "source": [
    "# 🎯 基于5个隐私政策问题的专门RAG评估Pipeline\n",
    "print(\"🎯 创建基于5个隐私政策问题的专门RAG评估Pipeline...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators.document_mrr import DocumentMRREvaluator\n",
    "from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator\n",
    "from haystack.components.evaluators.sas_evaluator import SASEvaluator\n",
    "\n",
    "\n",
    "# 定义5个问题及其对应的ground truth格式\n",
    "PRIVACY_POLICY_QUESTIONS = {\n",
    "    'q1': {\n",
    "        'text': \"1. Does the app declare the collection of data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q2': {\n",
    "        'text': \"2. What type of data does it collect?\",\n",
    "        'type': 'open'  # 开放题，groundtruth 为自由文本\n",
    "    },\n",
    "    'q3': {\n",
    "        'text': \"3. Does the app declare the purpose of data collection and use?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q4': {\n",
    "        'text': \"4. Can you opt out of data collection or delete data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q5': {\n",
    "        'text': \"5. Does the app share data with third parties?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q6': {\n",
    "        'text': \"6. If the app shares data with third parties, what third parties does the app share data with?\",\n",
    "        'type': 'open'  # 开放题，groundtruth 为自由文本\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_groundtruth_data(groundtruth_path='../groundtruth.json'):\n",
    "    \"\"\"加载groundtruth数据\"\"\"\n",
    "    with open(groundtruth_path, 'r', encoding='utf-8') as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # 转换为更易用的格式\n",
    "    gt_dict = {}\n",
    "    for item in gt_data:\n",
    "        app_id = item['id']\n",
    "        gt_dict[app_id] = {\n",
    "            'q1': item['q1'],\n",
    "            'q2': item['q2'], \n",
    "            'q3': item['q3'],\n",
    "            'q4': item['q4'],\n",
    "            'q5': item['q5'],\n",
    "            'q6': item['q6']\n",
    "        }\n",
    "    \n",
    "    print(f\"✅ 加载了 {len(gt_dict)} 个应用的groundtruth标注\")\n",
    "    return gt_dict\n",
    "\n",
    "def load_rag_output(app_id, outputs_dir='outputs'):\n",
    "    \"\"\"加载单个应用的RAG输出\"\"\"\n",
    "    output_file = os.path.join(outputs_dir, f\"{app_id}.json\")\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"⚠️ 输出文件不存在: {output_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            rag_data = json.load(f)\n",
    "        return rag_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载RAG输出失败 {output_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_question_mapping(rag_outputs):\n",
    "    \"\"\"从RAG输出中提取问题映射（按题号前缀 1.~7. 映射）\"\"\"\n",
    "    question_mapping = {}\n",
    "    for i, output in enumerate(rag_outputs):\n",
    "        try:\n",
    "            qtext = output['reply']['question'].strip()\n",
    "            if qtext.startswith(\"1.\"):\n",
    "                question_mapping['q1'] = output\n",
    "            elif qtext.startswith(\"2.\"):\n",
    "                question_mapping['q2'] = output\n",
    "            elif qtext.startswith(\"3.\"):\n",
    "                question_mapping['q3'] = output\n",
    "            elif qtext.startswith(\"4.\"):\n",
    "                question_mapping['q4'] = output\n",
    "            elif qtext.startswith(\"5.\"):\n",
    "                question_mapping['q5'] = output\n",
    "            elif qtext.startswith(\"6.\"):\n",
    "                question_mapping['q6'] = output\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 处理第{i+1}个输出时出错: {e}\")\n",
    "            continue\n",
    "    return question_mapping\n",
    "\n",
    "def create_privacy_evaluation_pipeline():\n",
    "    \"\"\"创建仅包含 Faithfulness / SAS / Context Relevance 的评估 pipeline\"\"\"\n",
    "    eval_pipeline = Pipeline()\n",
    "    eval_pipeline.add_component(\"faithfulness\", FaithfulnessEvaluator())\n",
    "    eval_pipeline.add_component(\"sas_evaluator\", SASEvaluator(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    eval_pipeline.add_component(\"context_relevance\", ContextRelevanceEvaluator())\n",
    "    return eval_pipeline\n",
    "\n",
    "def evaluate_single_app(app_id, gt_dict, outputs_dir='outputs'):\n",
    "    \"\"\"评估单个应用\"\"\"\n",
    "    print(f\"\\n📋 评估应用 ID: {app_id}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    gt_answers = gt_dict.get(app_id)\n",
    "    if not gt_answers:\n",
    "        print(f\"❌ 未找到应用 {app_id} 的groundtruth\")\n",
    "        return None\n",
    "    \n",
    "    rag_outputs = load_rag_output(app_id, outputs_dir)\n",
    "    if not rag_outputs:\n",
    "        print(f\"❌ 未找到应用 {app_id} 的RAG输出\")  \n",
    "        return None\n",
    "    \n",
    "    # 提取问题映射\n",
    "    question_mapping = extract_question_mapping(rag_outputs)\n",
    "    print(f\"✅ 找到 {len(question_mapping)} 个问题的输出\")\n",
    "    \n",
    "    # 评估结果\n",
    "    evaluation_results = {\n",
    "        'app_id': app_id,\n",
    "        'questions': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # 逐个问题评估\n",
    "    questions_data = []  # 用于pipeline评估\n",
    "    contexts_data = []\n",
    "    predicted_answers = []\n",
    "    ground_truth_answers = []\n",
    "    \n",
    "    bin_total = 0\n",
    "    bin_correct = 0\n",
    "\n",
    "    for q_key in ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']:\n",
    "        qconf = PRIVACY_POLICY_QUESTIONS[q_key]\n",
    "        qtype = qconf.get('type', 'binary')\n",
    "\n",
    "        if q_key in question_mapping:\n",
    "            rag_output = question_mapping[q_key]\n",
    "            try:\n",
    "                predicted_simple = rag_output['reply']['answer'].get('simple_answer', '')\n",
    "                predicted_full = rag_output['reply']['answer'].get('full_answer', '')\n",
    "                reference = rag_output['reply'].get('reference', '')\n",
    "\n",
    "                # ground truth\n",
    "                if qtype == 'binary':\n",
    "                    gt_label = (gt_answers[q_key] or '').strip().lower()  # 'y'/'n'\n",
    "                    gt_answer = qconf['expected_simple_answers'][gt_label]\n",
    "                    pred_norm = 'y' if predicted_simple.lower().startswith('y') else 'n'\n",
    "                    correct = (pred_norm == gt_label)\n",
    "                    bin_total += 1\n",
    "                    bin_correct += 1 if correct else 0\n",
    "                else:\n",
    "                    # 开放题直接用文本 groundtruth\n",
    "                    gt_answer = gt_answers[q_key]\n",
    "                    pred_norm = None\n",
    "                    correct = None  # 开放题不参与二分类准确率\n",
    "\n",
    "                evaluation_results['questions'][q_key] = {\n",
    "                    'question_text': qconf['text'],\n",
    "                    'type': qtype,\n",
    "                    'ground_truth_answer': gt_answer,\n",
    "                    'predicted_simple': predicted_simple,\n",
    "                    'predicted_full': predicted_full,\n",
    "                    'predicted_normalized': pred_norm,\n",
    "                    'correct': correct,\n",
    "                    'reference': reference\n",
    "                }\n",
    "\n",
    "                # 评估输入（SAS/Faithfulness/Context Relevance）\n",
    "                questions_data.append(qconf['text'])\n",
    "                # 优先使用 top-level 或 reply 中的 source_documents 的 excerpt/content 作为 context\n",
    "                src_docs = rag_output.get('source_documents') or rag_output.get('reply', {}).get('source_documents') or []\n",
    "                if src_docs:\n",
    "                    # 拼接多段以提高上下文覆盖\n",
    "                    excerpts = []\n",
    "                    for sd in src_docs:\n",
    "                        ex = sd.get('excerpt') or sd.get('content') or sd.get('text') or ''\n",
    "                        if ex:\n",
    "                            excerpts.append(ex)\n",
    "                    context_text = '\\n\\n'.join(excerpts) if excerpts else (reference or 'No relevant content found')\n",
    "                    contexts_data.append([context_text])\n",
    "                else:\n",
    "                    contexts_data.append([reference] if reference else ['No relevant content found'])\n",
    "\n",
    "                # 分开为 Faithfulness 与 SAS 准备预测答案：\n",
    "                # - faithfulness 使用 full_answer（更易判断证据支持）\n",
    "                # - sas 使用 simple_answer 对于 binary，开放题仍用 full_answer\n",
    "                if 'predicted_answers_faithfulness' not in locals():\n",
    "                    predicted_answers_faithfulness = []\n",
    "                if 'predicted_answers_sas' not in locals():\n",
    "                    predicted_answers_sas = []\n",
    "\n",
    "                predicted_answers_faithfulness.append(predicted_full)\n",
    "                if qtype == 'binary':\n",
    "                    predicted_answers_sas.append(predicted_simple)\n",
    "                else:\n",
    "                    predicted_answers_sas.append(predicted_full)\n",
    "\n",
    "                ground_truth_answers.append(gt_answer)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 处理问题 {q_key} 时出错: {e}\")\n",
    "                evaluation_results['questions'][q_key] = {\n",
    "                    'question_text': qconf['text'],\n",
    "                    'type': qtype,\n",
    "                    'ground_truth_answer': gt_answers.get(q_key, ''),\n",
    "                    'error': str(e),\n",
    "                    'correct': False if qtype == 'binary' else None\n",
    "                }\n",
    "        else:\n",
    "            print(f\"⚠️ 未找到问题 {q_key} 的RAG输出\")\n",
    "            evaluation_results['questions'][q_key] = {\n",
    "                'question_text': qconf['text'],\n",
    "                'type': qtype,\n",
    "                'missing': True,\n",
    "                'correct': False if qtype == 'binary' else None\n",
    "            }\n",
    "\n",
    "    # 准确率（仅二分类）\n",
    "    accuracy = (bin_correct / bin_total) if bin_total > 0 else 0.0\n",
    "    evaluation_results['summary'] = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct_count': bin_correct,\n",
    "        'total_count': bin_total,\n",
    "        'questions_found': len(question_mapping)\n",
    "    }\n",
    "\n",
    "    predicted_answers_faithfulness = locals().get('predicted_answers_faithfulness', [])\n",
    "    predicted_answers_sas = locals().get('predicted_answers_sas', [])\n",
    "    # 兼容旧代码：predicted_answers 保持非空表示可以运行评估\n",
    "    predicted_answers = predicted_answers_faithfulness or predicted_answers_sas or []\n",
    "    \n",
    "    # 运行Haystack评估pipeline（如果有数据）\n",
    "    if questions_data and predicted_answers:\n",
    "        try:\n",
    "            print(\"🎯 运行Haystack评估pipeline...\")\n",
    "            eval_pipeline = create_privacy_evaluation_pipeline()\n",
    "            \n",
    "            # 准备文档数据（从reference中提取）\n",
    "            retrieved_docs = []\n",
    "            ground_truth_docs = []\n",
    "            \n",
    "            for i, context_list in enumerate(contexts_data):\n",
    "                from haystack import Document\n",
    "                \n",
    "                # 创建检索文档\n",
    "                if context_list and context_list[0] != 'No context available':\n",
    "                    doc = Document(content=context_list[0], meta={\"source\": \"privacy_policy\"})\n",
    "                    retrieved_docs.append([doc])\n",
    "                    ground_truth_docs.append(doc)\n",
    "                else:\n",
    "                    empty_doc = Document(content=\"No relevant content found\", meta={\"source\": \"empty\"})\n",
    "                    retrieved_docs.append([empty_doc])\n",
    "                    ground_truth_docs.append(empty_doc)\n",
    "            \n",
    "            # 运行评估\n",
    "            haystack_results = eval_pipeline.run({\n",
    "                \"faithfulness\": {\n",
    "                    \"questions\": questions_data,\n",
    "                    \"contexts\": contexts_data,\n",
    "                    \"predicted_answers\": predicted_answers_faithfulness,\n",
    "                },\n",
    "                \"sas_evaluator\": {\n",
    "                    \"predicted_answers\": predicted_answers_sas,\n",
    "                    \"ground_truth_answers\": ground_truth_answers\n",
    "                },\n",
    "                \"context_relevance\": {\n",
    "                    \"questions\": questions_data,\n",
    "                    \"contexts\": contexts_data\n",
    "                },\n",
    "            })\n",
    "            \n",
    "            # 添加Haystack评估结果\n",
    "            evaluation_results['haystack_metrics'] = {\n",
    "                'faithfulness': {\n",
    "                    'individual_scores': haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"faithfulness\", {}).get(\"individual_scores\") else 0\n",
    "                },\n",
    "                'sas': {\n",
    "                    'individual_scores': haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"sas_evaluator\", {}).get(\"individual_scores\") else 0\n",
    "                },\n",
    "                'context_relevance': {\n",
    "                    'individual_scores': haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", []),\n",
    "                    'average': (\n",
    "                        sum(haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", [])) /\n",
    "                        len(haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\", [1]))\n",
    "                    ) if haystack_results.get(\"context_relevance\", {}).get(\"individual_scores\") else 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(\"✅ Haystack评估完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Haystack评估失败: {e}\")\n",
    "            evaluation_results['haystack_metrics'] = {'error': str(e)}\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def run_comprehensive_privacy_evaluation():\n",
    "    \"\"\"运行完整的隐私政策评估\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 开始隐私政策RAG系统综合评估\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 加载groundtruth数据\n",
    "    gt_dict = load_groundtruth_data()\n",
    "    \n",
    "    # 获取所有有输出文件的应用ID\n",
    "    outputs_dir = 'outputs'\n",
    "    available_apps = []\n",
    "    \n",
    "    if os.path.exists(outputs_dir):\n",
    "        for filename in os.listdir(outputs_dir):\n",
    "            if filename.endswith('.json') and filename.replace('.json', '').isdigit():\n",
    "                app_id = int(filename.replace('.json', ''))\n",
    "                if app_id in gt_dict:\n",
    "                    available_apps.append(app_id)\n",
    "    \n",
    "    print(f\"📊 找到 {len(available_apps)} 个可评估的应用\")\n",
    "    \n",
    "    if not available_apps:\n",
    "        print(\"❌ 没有找到可评估的应用！\")\n",
    "        return\n",
    "    \n",
    "    # 选择要评估的应用（这里评估前3个作为示例）\n",
    "    apps_to_evaluate = available_apps  # 可以修改数量\n",
    "    print(f\"🎯 将评估前 {len(apps_to_evaluate)} 个应用: {apps_to_evaluate}\")\n",
    "    \n",
    "    # 逐个评估应用\n",
    "    all_results = []\n",
    "    overall_stats = {\n",
    "        'total_apps': 0,\n",
    "        'total_questions': 0,\n",
    "        'total_correct': 0,\n",
    "        'per_question_stats': {q: {'correct': 0, 'total': 0} for q in ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']},\n",
    "        'haystack_aggregated': {\n",
    "            'faithfulness_scores': [],\n",
    "            'sas_scores': [],\n",
    "            'context_relevance_scores': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for app_id in apps_to_evaluate:\n",
    "        result = evaluate_single_app(app_id, gt_dict, outputs_dir)\n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # 更新总体统计\n",
    "            overall_stats['total_apps'] += 1\n",
    "            overall_stats['total_questions'] += result['summary']['total_count']\n",
    "            overall_stats['total_correct'] += result['summary']['correct_count']\n",
    "            \n",
    "            # 更新各问题统计\n",
    "            for q_key, q_data in result['questions'].items():\n",
    "                overall_stats['per_question_stats'][q_key]['total'] += 1\n",
    "                if q_data.get('correct', False):\n",
    "                    overall_stats['per_question_stats'][q_key]['correct'] += 1\n",
    "            \n",
    "            # 聚合Haystack评估结果\n",
    "            if 'haystack_metrics' in result and 'error' not in result['haystack_metrics']:\n",
    "                hm = result['haystack_metrics']\n",
    "                if hm.get('faithfulness', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['faithfulness_scores'].extend(\n",
    "                        hm['faithfulness']['individual_scores']\n",
    "                    )\n",
    "                if hm.get('sas', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['sas_scores'].extend(\n",
    "                        hm['sas']['individual_scores']\n",
    "                    )\n",
    "                if hm.get('context_relevance', {}).get('individual_scores'):\n",
    "                    overall_stats['haystack_aggregated']['context_relevance_scores'].extend(\n",
    "                        hm['context_relevance']['individual_scores']\n",
    "                    )\n",
    "    \n",
    "    # 计算最终统计\n",
    "    final_results = {\n",
    "        'metadata': {\n",
    "            'evaluation_date': str(pd.Timestamp.now()),\n",
    "            'total_apps_evaluated': overall_stats['total_apps'],\n",
    "            'total_questions_evaluated': overall_stats['total_questions'],\n",
    "            'questions_definition': PRIVACY_POLICY_QUESTIONS\n",
    "        },\n",
    "        'classification_metrics': {\n",
    "            'overall_accuracy': overall_stats['total_correct'] / overall_stats['total_questions'] if overall_stats['total_questions'] > 0 else 0,\n",
    "            'per_question_accuracy': {\n",
    "                q: stats['correct'] / stats['total'] if stats['total'] > 0 else 0 \n",
    "                for q, stats in overall_stats['per_question_stats'].items()\n",
    "            }\n",
    "        },\n",
    "        'haystack_metrics': {\n",
    "            'faithfulness_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['faithfulness_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['faithfulness_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['faithfulness_scores'] else 0,\n",
    "            'sas_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['sas_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['sas_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['sas_scores'] else 0,\n",
    "            'context_relevance_average': (\n",
    "                sum(overall_stats['haystack_aggregated']['context_relevance_scores']) /\n",
    "                len(overall_stats['haystack_aggregated']['context_relevance_scores'])\n",
    "            ) if overall_stats['haystack_aggregated']['context_relevance_scores'] else 0\n",
    "        },\n",
    "        'detailed_results': all_results\n",
    "    }\n",
    "    \n",
    "    # 显示结果\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 隐私政策RAG评估结果\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"📈 总体准确率: {final_results['classification_metrics']['overall_accuracy']:.3f}\")\n",
    "    print(f\"📊 评估了 {final_results['metadata']['total_apps_evaluated']} 个应用，{final_results['metadata']['total_questions_evaluated']} 个问题\")\n",
    "    \n",
    "    print(\"\\n🔍 各问题准确率:\")\n",
    "    for q_key, accuracy in final_results['classification_metrics']['per_question_accuracy'].items():\n",
    "        q_text = PRIVACY_POLICY_QUESTIONS[q_key]['text'][:50] + \"...\"\n",
    "        print(f\"  {q_key}: {accuracy:.3f} - {q_text}\")\n",
    "    \n",
    "    if final_results['haystack_metrics']['faithfulness_average'] > 0:\n",
    "        print(f\"\\n🎯 Haystack评估指标:\")\n",
    "        print(f\"  🔍 Faithfulness: {final_results['haystack_metrics']['faithfulness_average']:.3f}\")\n",
    "        print(f\"  📝 SAS: {final_results['haystack_metrics']['sas_average']:.3f}\")\n",
    "        print(f\"  📎 Context Relevance: {final_results['haystack_metrics']['context_relevance_average']:.3f}\")\n",
    "    \n",
    "    # 保存结果\n",
    "    output_path = 'eval/privacy_policy_rag_evaluation.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 详细评估结果已保存至: {output_path}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# 执行评估\n",
    "print(\"🎯 开始执行隐私政策RAG评估...\")\n",
    "evaluation_results = run_comprehensive_privacy_evaluation()\n",
    "\n",
    "print(\"\\n🎉 隐私政策RAG评估完成！\")\n",
    "print(\"📋 评估涵盖:\")\n",
    "print(\"   ✅ 5个隐私政策核心问题的分类准确率\")\n",
    "print(\"   🔍 Haystack专业评估指标 (Faithfulness, SAS, Context Relevance)\")\n",
    "print(\"   📊 详细的逐应用、逐问题分析\")\n",
    "print(\"   💾 完整结果保存为JSON格式\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
