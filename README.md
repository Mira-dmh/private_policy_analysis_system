# RAG Pipeline with Incremental Indexing

This is a Haystack-based Retrieval Augmented Generation (RAG) project for structured Q&A and evaluation of app privacy policies, supporting incremental indexing, reranking, and automated evaluation.

## ğŸš€ Quick Start (Windows PowerShell)

### 1) Requirements
- Python 3.8+ (recommended 3.10/3.11)
- pip

Recommend using virtual environment to isolate dependencies:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate
```

### 2) Install Dependencies

- Recommended (simplified dependencies):

```powershell
pip install -r requirements\requirements_simple.txt
```

- Full dependencies:

```powershell
pip install -r requirements\requirements.txt
```

- Or manually install core packages (if needed):

```powershell
pip install haystack-ai "datasets>=3.6.0" "sentence-transformers>=4.1.0" accelerate python-dotenv tqdm cohere openai
```

### 3) Configure Environment Variables

Place a `.env` file in the repository root or `new version/` directory (Notebook will automatically load the nearest `.env`):

```env
COHERE_API_KEY=your_actual_cohere_api_key
OPENAI_API_KEY=your_actual_openai_api_key
# Optional: if using custom gateway
CO_API_URL=https://api.cohere.ai/v1
```

If the repository provides `.env.template`, you can copy one:

```powershell
copy .env.template .env
```

### 4) Verify Environment (Optional)

The repository provides a verification script: `requirements/verify_setup.py`. Note: Some path checks in the script are based on old structure and may report missing Notebooks but don't affect operation.

```powershell
python requirements\verify_setup.py
```

### 5) Run Notebook

Notebook location: `new version/incremental_indexing_rag .ipynb` (note the space before `.ipynb` in filename).

```powershell
jupyter notebook "new version/incremental_indexing_rag .ipynb"
```

In the Notebook:
- Ensure the first two cells have completed dependency installation and API Key loading.
- To control processing quantity, modify variable `num_to_process` (default 1).

## ğŸ“ Key Directories and Files

```
POLICY-ANALYSIS/
â”œâ”€â”€ files/
â”‚   â””â”€â”€ index_table.json                # Input list (contains id and privacy policy URLs)
â”œâ”€â”€ new version/
â”‚   â”œâ”€â”€ incremental_indexing_rag .ipynb # Main workflow Notebook (incremental indexing + RAG + evaluation)
â”‚   â”œâ”€â”€ outputs/                        # RAG outputs generated by Notebook (one JSON per app_id)
â”‚   â””â”€â”€ eval/                           # Evaluation results output (privacy_policy_rag_evaluation.json)
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ requirements_simple.txt
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ INSTALLATION.md
â”‚   â””â”€â”€ verify_setup.py
â”œâ”€â”€ groundtruth.json                    # Evaluation annotations (q1~q6)
â””â”€â”€ README.md
```

Note: The Notebook internally reads input from `../files/index_table.json`, so it's recommended to maintain input files in the `files/` directory.

## ğŸ”§ Main Features

- Incremental Indexing: URL by URL crawling â†’ convert to documents â†’ cleaning â†’ chunking â†’ embedding â†’ write to temporary in-memory store
- Semantic Retrieval: Cohere document/query embedding + InMemory retrieval
- Reranking: Cohere rerank-english-v3.0, semantic reranking of recalled results
- Generation: OpenAI GPT generates strict JSON structure answers (built-in JSON repair fallback)
- Evaluation: Faithfulness / SAS / Context Relevance metrics + binary classification accuracy

## ğŸ“‹ Input Data Format

`files/index_table.json` example (current actual format in repository):

```json
[
  { "content": "", "id": 1361356590, "url": "https://example.com/privacy" },
  { "content": "", "id": 1493155192, "url": "https://example.com/policy" }
]
```

Field descriptions:
- id: Integer app ID (filenames are named with this ID)
- url: Privacy policy page URL
- content: Can be left empty (crawler will fetch online)

## â“ Predefined Questions (6 total)

The Notebook will answer the following 6 questions for each application by default:

1. Does the app declare the collection of data?
2. If the app declares the collection of data, what type of data does it collect?
3. Does the app declare the purpose of data collection and use?
4. Can the user opt out of data collection or delete data?
5. Does the app share data with third parties?
6. If the app shares data with third parties, what third parties does the app share data with?

Where q1/q3/q4/q5 are binary classification (Yes/No), q2/q6 are open-ended questions, answer structures are differentiated (e.g., q2/q6's simple_answer can be NOTUSED, see Notebook Prompt constraints for details).

## ğŸ“„ Output and Evaluation

- Generated answers: `new version/outputs/{app_id}.json`
- Evaluation results: `new version/eval/privacy_policy_rag_evaluation.json`

Single application output file is an array containing 6 records, each corresponding to one question, core structure as follows (excerpt):

```json
{
  "meta": { "id": 1361356590, "url": "https://example.com/privacy", "title": "Example App" },
  "reply": {
    "qid": "q1",
    "question": "1. Does the app declare the collection of data?",
    "answer": {
      "full_answer": "...",
      "simple_answer": "Yes",
      "extended_simple_answer": { "comment": "", "content": "" }
    },
    "analysis": "...",
    "reference": "Original text snippet + URL"
  },
  "source_documents": [
    { "id": "...", "score": 0.87, "excerpt": "...", "url": "..." }
  ]
}
```

Note:
- Notebook has built-in JSON parsing fallback logic to maximally repair occasional non-strict JSON output from models.
- `source_documents` is optional, recording evidence snippet summaries used for answering.

## ğŸ”„ Running Steps Overview

1) Prepare `files/index_table.json` and `.env`
2) Open and run `new version/incremental_indexing_rag .ipynb`
3) To reduce testing time, first set `num_to_process = 1` and only verify the first URL
4) Check generated `{app_id}.json` in `new version/outputs/`
5) Continue running evaluation cells below, results in `new version/eval/`

## ğŸ› ï¸ Troubleshooting (FAQ)

1) Cannot crawl pages (403/404/timeout)
- Notebook has common variant fallbacks for URLs (http/https, trailing slash, common privacy paths). If still failing:
  - Check if URL can be opened in browser
  - In network environments requiring proxy, configure system/terminal proxy
  - Some sites may need alternative entry pages (write actual privacy page URL into `files/index_table.json`)

2) NLTK download failures or slow
- Punkt resources downloaded by `nltk.download('punkt')`, can pre-install offline or configure proxy when network is restricted.

3) API quota or authentication failures
- Confirm keys in `.env` are valid and match current account/region
- Both OpenAI/Cohere may error due to quota exceeded, appropriately reduce concurrency or processing quantity

4) Evaluation stage errors
- Confirm `groundtruth.json` and generated `{app_id}.json` question numbers and text can be correctly matched (6 questions listed in README)
- If `requirements/verify_setup.py` indicates missing Notebook paths, can ignore that item (script based on old path conventions)

5) Memory usage
- This project uses independent in-memory document stores for each application and reuses Pipeline, usually stable memory usage; if still under pressure, can reduce `top_k`, shorten chunk length, or run in batches.

## ğŸ“š Tech Stack

- Framework: Haystack 2.x
- Embedding: Cohere embed-english-v3.0 (documents and queries)
- Reranking: Cohere rerank-english-v3.0
- Generation: OpenAI GPT-3.5-turbo
- Evaluation: Faithfulness / SAS / Context Relevance

## ğŸ¤ Contributing

Welcome to submit Issues / PRs to improve the project (e.g., update verification script paths to new structure, add command line entry, add cached document storage, etc.).

## ğŸ“„ License

This project follows the terms of the corresponding open source license.

---

Important Notice: Using this project requires valid Cohere and OpenAI API Keys, please comply with terms of service and billing policies.
